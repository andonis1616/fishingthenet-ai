{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import email\n",
    "from email.policy import default\n",
    "import csv\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "def getcharsets(msg):\n",
    "    charsets = set({})\n",
    "    for c in msg.get_charsets():\n",
    "        if c is not None:\n",
    "            charsets.update([c])\n",
    "    return charsets\n",
    "\n",
    "def handleerror(errmsg, emailmsg,cs):\n",
    "    print()\n",
    "    print(errmsg)\n",
    "    print(\"This error occurred while decoding with \",cs,\" charset.\")\n",
    "    print(\"These charsets were found in the one email.\",getcharsets(emailmsg))\n",
    "    print(\"This is the subject:\",emailmsg['subject'])\n",
    "    print(\"This is the sender:\",emailmsg['From'])\n",
    "\n",
    "def getbodyfromemail(msg):\n",
    "    body = None\n",
    "    #Walk through the parts of the email to find the text body.    \n",
    "    if msg.is_multipart():    \n",
    "        for part in msg.walk():\n",
    "\n",
    "            # If part is multipart, walk through the subparts.            \n",
    "            if part.is_multipart(): \n",
    "\n",
    "                for subpart in part.walk():\n",
    "                    if subpart.get_content_type() == 'text/plain':\n",
    "                        # Get the subpart payload (i.e the message body)\n",
    "                        body = subpart.get_payload(decode=True) \n",
    "                        #charset = subpart.get_charset()\n",
    "\n",
    "            # Part isn't multipart so get the email body\n",
    "            elif part.get_content_type() == 'text/plain':\n",
    "                body = part.get_payload(decode=True)\n",
    "                #charset = part.get_charset()\n",
    "\n",
    "    # If this isn't a multi-part message then get the payload (i.e the message body)\n",
    "    elif msg.get_content_type() == 'text/plain':\n",
    "        body = msg.get_payload(decode=True) \n",
    "\n",
    "   # No checking done to match the charset with the correct part. \n",
    "    for charset in getcharsets(msg):\n",
    "        try:\n",
    "            body = body.decode(charset)\n",
    "        except UnicodeDecodeError:\n",
    "            handleerror(\"UnicodeDecodeError: encountered.\",msg,charset)\n",
    "        except AttributeError:\n",
    "             handleerror(\"AttributeError: encountered\" ,msg,charset)\n",
    "    return body    \n",
    "\n",
    "\n",
    "class MboxReader:\n",
    "    def __init__(self, filename):\n",
    "        self.handle = open(filename, 'rb')\n",
    "        assert self.handle.readline().startswith(b'From ')\n",
    "\n",
    "    def __enter__(self):\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, exc_type, exc_value, exc_traceback):\n",
    "        self.handle.close()\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self.__next__())\n",
    "\n",
    "    def __next__(self):\n",
    "        lines = []\n",
    "        while True:\n",
    "            line = self.handle.readline()\n",
    "            if line == b'' or line.startswith(b'From '):\n",
    "                yield email.message_from_bytes(b''.join(lines), policy=default)\n",
    "                if line == b'':\n",
    "                    break\n",
    "                lines = []\n",
    "                continue\n",
    "            lines.append(line)\n",
    "\n",
    "# ['Message-ID', 'Date', 'From', 'To', 'Subject', 'Mime-Version', 'Content-Type', 'Content-Transfer-Encoding', 'X-From', 'X-To', 'X-cc', 'X-bcc', 'X-Folder', 'X-Origin', 'X-FileName']\n",
    "header = ['From', 'Subject', 'Body', 'X-FileName', 'IsPhishing']\n",
    "with open('enron-emails-bag.csv', 'w', encoding='UTF8') as f:\n",
    "    writer = csv.writer(f)\n",
    "\n",
    "    writer.writerow(header)\n",
    "    with MboxReader(\"emails-enron.mbox\") as mbox:\n",
    "        for message in mbox:\n",
    "            if message.get('From'):\n",
    "                sender = str(message.get('From'))\n",
    "            else:\n",
    "                sender = \"0\"\n",
    "            \n",
    "            if message.get('Subject'):\n",
    "                subject = str(message.get('Subject'))\n",
    "            else:\n",
    "                subject = \"0\"\n",
    "            \n",
    "            if getbodyfromemail(message):\n",
    "                CountVec = CountVectorizer(ngram_range=(1,1), stop_words='english')\n",
    "                try:\n",
    "                    Count_data = CountVec.fit_transform([str(getbodyfromemail(message))])\n",
    "                    bagofwords = pd.DataFrame(Count_data.toarray(),columns=CountVec.get_feature_names_out()).to_dict()\n",
    "                    newbagofwords = {}\n",
    "                    for bag in bagofwords:\n",
    "                        newbagofwords[bag] = bagofwords[bag][0]\n",
    "                    body = newbagofwords\n",
    "                except:\n",
    "                    body = \"0\"\n",
    "                # body = str(getbodyfromemail(message))\n",
    "            else:\n",
    "                body = \"0\"\n",
    "\n",
    "            if message.get('X-FileName'):\n",
    "                filename =  str(message.get('X-FileName'))\n",
    "            else:\n",
    "                filename = \"0\"\n",
    "            \n",
    "            data = [sender, subject, body, filename, 0]\n",
    "\n",
    "            writer.writerow(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import email\n",
    "from email.policy import default\n",
    "import csv\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "def getcharsets(msg):\n",
    "    charsets = set({})\n",
    "    for c in msg.get_charsets():\n",
    "        if c is not None:\n",
    "            charsets.update([c])\n",
    "    return charsets\n",
    "\n",
    "def handleerror(errmsg, emailmsg,cs):\n",
    "    print()\n",
    "    print(errmsg)\n",
    "    print(\"This error occurred while decoding with \",cs,\" charset.\")\n",
    "    print(\"These charsets were found in the one email.\",getcharsets(emailmsg))\n",
    "    print(\"This is the subject:\",emailmsg['subject'])\n",
    "    print(\"This is the sender:\",emailmsg['From'])\n",
    "\n",
    "def getbodyfromemail(msg):\n",
    "    body = None\n",
    "    #Walk through the parts of the email to find the text body.    \n",
    "    if msg.is_multipart():    \n",
    "        for part in msg.walk():\n",
    "\n",
    "            # If part is multipart, walk through the subparts.            \n",
    "            if part.is_multipart(): \n",
    "\n",
    "                for subpart in part.walk():\n",
    "                    if subpart.get_content_type() == 'text/plain':\n",
    "                        # Get the subpart payload (i.e the message body)\n",
    "                        body = subpart.get_payload(decode=True) \n",
    "                        #charset = subpart.get_charset()\n",
    "\n",
    "            # Part isn't multipart so get the email body\n",
    "            elif part.get_content_type() == 'text/plain':\n",
    "                body = part.get_payload(decode=True)\n",
    "                #charset = part.get_charset()\n",
    "\n",
    "    # If this isn't a multi-part message then get the payload (i.e the message body)\n",
    "    elif msg.get_content_type() == 'text/plain':\n",
    "        body = msg.get_payload(decode=True) \n",
    "\n",
    "   # No checking done to match the charset with the correct part. \n",
    "    for charset in getcharsets(msg):\n",
    "        try:\n",
    "            body = body.decode(charset)\n",
    "        except:\n",
    "            return \"None\"\n",
    "    return body    \n",
    "\n",
    "\n",
    "class MboxReader:\n",
    "    def __init__(self, filename):\n",
    "        self.handle = open(filename, 'rb')\n",
    "        assert self.handle.readline().startswith(b'From ')\n",
    "\n",
    "    def __enter__(self):\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, exc_type, exc_value, exc_traceback):\n",
    "        self.handle.close()\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self.__next__())\n",
    "\n",
    "    def __next__(self):\n",
    "        lines = []\n",
    "        while True:\n",
    "            line = self.handle.readline()\n",
    "            if line == b'' or line.startswith(b'From '):\n",
    "                yield email.message_from_bytes(b''.join(lines), policy=default)\n",
    "                if line == b'':\n",
    "                    break\n",
    "                lines = []\n",
    "                continue\n",
    "            lines.append(line)\n",
    "\n",
    "# ['Message-ID', 'Date', 'From', 'To', 'Subject', 'Mime-Version', 'Content-Type', 'Content-Transfer-Encoding', 'X-From', 'X-To', 'X-cc', 'X-bcc', 'X-Folder', 'X-Origin', 'X-FileName']\n",
    "header = ['From', 'Subject', 'Body', 'X-FileName', 'IsPhishing']\n",
    "with open('enron-phishing.csv', 'w', encoding='UTF8') as f:\n",
    "    writer = csv.writer(f)\n",
    "\n",
    "    writer.writerow(header)\n",
    "    with MboxReader(\"emails-phishing.mbox\") as mbox:\n",
    "        for message in mbox:\n",
    "            # print(str(getbodyfromemail(message)))\n",
    "            if message.get('From'):\n",
    "                sender = str(message.get('From'))\n",
    "            else:\n",
    "                sender = \"None\"\n",
    "            \n",
    "            if message.get('Subject'):\n",
    "                subject = str(message.get('Subject'))\n",
    "            else:\n",
    "                subject = \"None\"\n",
    "            \n",
    "            if getbodyfromemail(message):\n",
    "                # CountVec = CountVectorizer(ngram_range=(1,1), stop_words='english')\n",
    "                # try:\n",
    "                #     Count_data = CountVec.fit_transform([str(getbodyfromemail(message))])\n",
    "                #     bagofwords = pd.DataFrame(Count_data.toarray(),columns=CountVec.get_feature_names_out()).to_dict()\n",
    "                #     newbagofwords = {}\n",
    "                #     for bag in bagofwords:\n",
    "                #         newbagofwords[bag] = bagofwords[bag][0]\n",
    "                #     body = newbagofwords\n",
    "                # except:\n",
    "                #     body = \"None\"\n",
    "                body = str(getbodyfromemail(message))\n",
    "            else:\n",
    "                body = \"None\"\n",
    "\n",
    "            if message.get('X-FileName'):\n",
    "                filename =  str(message.get('X-FileName'))\n",
    "            else:\n",
    "                filename = \"None\"\n",
    "            \n",
    "            data = [sender, subject, body, filename, 1]\n",
    "\n",
    "            writer.writerow(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3423\n",
      "428\n",
      "428\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Andrei\\AppData\\Local\\Temp\\ipykernel_24616\\528272447.py:14: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
      "  df = {key: value[:,tf.newaxis] for key, value in dataframe.items()}\n",
      "C:\\Users\\Andrei\\AppData\\Local\\Temp\\ipykernel_24616\\528272447.py:14: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
      "  df = {key: value[:,tf.newaxis] for key, value in dataframe.items()}\n",
      "C:\\Users\\Andrei\\AppData\\Local\\Temp\\ipykernel_24616\\528272447.py:14: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
      "  df = {key: value[:,tf.newaxis] for key, value in dataframe.items()}\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"c:\\Users\\Andrei\\.conda\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 1727, in test_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\Andrei\\.conda\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 1713, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\Andrei\\.conda\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 1701, in run_step  **\n        outputs = model.test_step(data)\n    File \"c:\\Users\\Andrei\\.conda\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 1665, in test_step\n        y_pred = self(x, training=False)\n    File \"c:\\Users\\Andrei\\.conda\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"c:\\Users\\Andrei\\.conda\\envs\\tf\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 197, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Missing data for input \"input_1\". You passed a data dictionary with keys ['From', 'Subject', 'Body', 'X-FileName', 'label']. Expected the following keys: ['input_1']\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [183], line 79\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[39m#model = tf.keras.models.Sequential ([\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \u001b[39m#    tf.keras.layers.Dense(16,activation='relu'),\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \u001b[39m#    tf.keras.layers.Dense(16,activation='relu'),\u001b[39;00m\n\u001b[0;32m     68\u001b[0m \u001b[39m#    tf.keras.layers.Dense(16,activation='softmax'),\u001b[39;00m\n\u001b[0;32m     69\u001b[0m \u001b[39m#    tf.keras.layers.Dense(1,activation=\"sigmoid\"),\u001b[39;00m\n\u001b[0;32m     70\u001b[0m \u001b[39m#])\u001b[39;00m\n\u001b[0;32m     74\u001b[0m model\u001b[39m.\u001b[39mcompile(optimizer\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39moptimizers\u001b[39m.\u001b[39mAdam(learning_rate\u001b[39m=\u001b[39m\u001b[39m0.001\u001b[39m),\n\u001b[0;32m     75\u001b[0m loss\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mlosses\u001b[39m.\u001b[39mBinaryCrossentropy(),\n\u001b[0;32m     76\u001b[0m metrics\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m---> 79\u001b[0m model\u001b[39m.\u001b[39;49mevaluate(train_data)\n\u001b[0;32m     80\u001b[0m \u001b[39m#model.evaluate(X_valid, Y_valid)\u001b[39;00m\n\u001b[0;32m     81\u001b[0m \u001b[39m#print (list(train_data)[0])\u001b[39;00m\n\u001b[0;32m     82\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[0;32m     87\u001b[0m \u001b[39m#\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Andrei\\.conda\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_fileqbb71818.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__test_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(step_function), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), ag__\u001b[39m.\u001b[39mld(iterator)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"c:\\Users\\Andrei\\.conda\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 1727, in test_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\Andrei\\.conda\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 1713, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\Andrei\\.conda\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 1701, in run_step  **\n        outputs = model.test_step(data)\n    File \"c:\\Users\\Andrei\\.conda\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 1665, in test_step\n        y_pred = self(x, training=False)\n    File \"c:\\Users\\Andrei\\.conda\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"c:\\Users\\Andrei\\.conda\\envs\\tf\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 197, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Missing data for input \"input_1\". You passed a data dictionary with keys ['From', 'Subject', 'Body', 'X-FileName', 'label']. Expected the following keys: ['input_1']\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def df_to_dataset (dataframe, shuffle=True, batch_size=1024):\n",
    "    df = dataframe.copy()\n",
    "    labels = df.pop(\"label\")\n",
    "    #df = {key: value[:,tf.newaxis] for key, value in dataframe.items()}\n",
    "    df = {key: value[:,tf.newaxis] for key, value in dataframe.items()}\n",
    "    ds = tf.data.Dataset.from_tensor_slices((dict(df),labels))\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(buffer_size=len(dataframe))\n",
    "    ds = ds.batch(batch_size)\n",
    "    ds = ds.prefetch(tf.data.AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "df = pd.read_csv(\"enron-emails-bag.csv\") \n",
    "\n",
    "#df.dropna(subset=[\"deva\",\"altceva\"])\n",
    "#label = []\n",
    "\n",
    "#for i in range(len(df.columns)):\n",
    "#    label.append(df.columns[i])\n",
    "\n",
    "#print (label)\n",
    "#print(df.head())\n",
    "\n",
    "df[\"label\"]=(df.IsPhishing).astype(int)\n",
    "df = df[[\"From\",\"Subject\",\"Body\",\"X-FileName\",\"label\"]]\n",
    "#X = df[df.columns[:-1]].values\n",
    "#Y = df[df.columns[-1]].values\n",
    "\n",
    "#print (X)\n",
    "#print(Y)\n",
    "\n",
    "#P1\n",
    "#X_train, X_temp, Y_train, Y_temp = train_test_split(X,Y,test_size=0.4,random_state=0)\n",
    "#X_valid, X_test, Y_valid, Y_test = train_test_split(X_temp,Y_temp,test_size=0.5,random_state=0)\n",
    "\n",
    "#P2 -> 80% training 10% valid 10% test\n",
    "train, val, test = np.split(df.sample(frac=1),[int(0.8*len(df)),int(0.9*len(df))])\n",
    "\n",
    "#len(train),len(val),len(test)\n",
    "print(len(train))\n",
    "print(len(val))\n",
    "print(len(test))\n",
    "\n",
    "\n",
    "train_data = df_to_dataset(train)\n",
    "valid_data = df_to_dataset(val)\n",
    "test_data = df_to_dataset(test)\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.Input(shape=(5,)))\n",
    "model.add(tf.keras.layers.Dense(16,activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(16,activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(16,activation='softmax'))\n",
    "model.add(tf.keras.layers.Dense(1,activation='sigmoid'))\n",
    "\n",
    "#model = tf.keras.models.Sequential ([\n",
    "#    tf.keras.layers.Dense(16,activation='relu'),\n",
    "#    tf.keras.layers.Dense(16,activation='relu'),\n",
    "#    tf.keras.layers.Dense(16,activation='softmax'),\n",
    "#    tf.keras.layers.Dense(1,activation=\"sigmoid\"),\n",
    "#])\n",
    "\n",
    "\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model.evaluate(train_data)\n",
    "#model.evaluate(X_valid, Y_valid)\n",
    "#print (list(train_data)[0])\n",
    "\n",
    "#plt.hist(df.points, bins=20)\n",
    "#plt.title(\"Points Histogram\")\n",
    "#...\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mailparser\n",
    "import mailbox\n",
    "import csv\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "mymail = mailbox.mbox(\"phsihingtrainset/phishing-2021\")\n",
    "\n",
    "def bagOfWords(body):\n",
    "    CountVec = CountVectorizer(ngram_range=(1,1), stop_words='english')\n",
    "    try:\n",
    "        Count_data = CountVec.fit_transform([body])\n",
    "        bagofwords = pd.DataFrame(Count_data.toarray(),columns=CountVec.get_feature_names_out()).to_dict()\n",
    "        newbagofwords = {}\n",
    "        for bag in bagofwords:\n",
    "            newbagofwords[bag] = bagofwords[bag][0]\n",
    "        return newbagofwords\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "header = ['From', 'Subject', 'Body', 'IsPhishing']\n",
    "with open('phishing-2021.csv', 'w', encoding='UTF8', newline=\"\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(header)\n",
    "    for message in mymail:\n",
    "        mail = mailparser.parse_from_string(str(message))\n",
    "        body = BeautifulSoup(mail.body).text\n",
    "        bow = bagOfWords(body)\n",
    "        sender = mail.from_\n",
    "        subject = mail.subject\n",
    "        data = [sender, subject, bow, 1]\n",
    "\n",
    "        writer.writerow(data)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('tf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a6400b930df6fcbaa5286d433e4a79fef72001e869af62a221c8686cd61c435a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
